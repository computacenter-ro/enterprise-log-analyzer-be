# =============================================================================
# Telegraf Configuration for Enterprise Log Analyzer
# =============================================================================
#
# This configuration collects logs and metrics from a macOS server running
# Docker Desktop and sends them to the Enterprise Log Analyzer backend.
#
# Environment Variables Required:
#   TELEGRAF_TOKEN    - Authentication token from /api/v1/sources
#   TELEGRAF_AGENT_ID - Agent ID from /api/v1/sources
#   BACKEND_URL       - Backend API URL (default: http://localhost:8000)
#
# Optional Environment Variables:
#   HOSTNAME          - Override hostname (default: system hostname)
#   COLLECTION_INTERVAL - Data collection interval (default: 10s)
#   FLUSH_INTERVAL    - Data flush interval (default: 10s)
#
# =============================================================================

# -----------------------------------------------------------------------------
# GLOBAL TAGS
# -----------------------------------------------------------------------------
# Tags applied to all metrics for identification and filtering
[global_tags]
  # Environment identifier (e.g., "development", "staging", "production")
  environment = "${ENVIRONMENT}"

  # Deployment region or datacenter
  region = "${REGION}"

  # Application or service group
  service_group = "${SERVICE_GROUP}"

# -----------------------------------------------------------------------------
# AGENT CONFIGURATION
# -----------------------------------------------------------------------------
[agent]
  # Collection interval for all inputs (can be overridden per-input)
  interval = "10s"

  # Round collection times to the interval
  round_interval = true

  # Maximum number of metrics to send per batch
  metric_batch_size = 1000

  # Maximum number of metrics buffered before dropping
  metric_buffer_limit = 10000

  # Jitter to add to collection to prevent thundering herd
  collection_jitter = "0s"

  # Flush interval for all outputs
  flush_interval = "10s"

  # Jitter added to flush interval
  flush_jitter = "0s"

  # Timestamp precision (nanosecond, microsecond, millisecond, second)
  precision = "0s"

  # Override default hostname
  hostname = ""

  # If true, do not set the "host" tag
  omit_hostname = false

  # Log level (debug, info, warn, error)
  debug = false
  quiet = false
  logtarget = "stderr"

# =============================================================================
# OUTPUT PLUGINS
# =============================================================================

# -----------------------------------------------------------------------------
# HTTP OUTPUT - Enterprise Log Analyzer Backend
# -----------------------------------------------------------------------------
# Primary output: sends all metrics and logs to the FastAPI backend
[[outputs.http]]
  # Backend telemetry endpoint
  url = "${BACKEND_URL}/api/v1/telemetry/telegraf"

  # Request timeout
  timeout = "10s"

  # HTTP method
  method = "POST"

  # Data serialization format
  data_format = "json"

  # Batch metrics in JSON array format
  # This matches the TelegrafBatch schema expected by the backend
  json_transformation = '''
    {
      "metrics": .
    }
  '''

  # HTTP headers for authentication
  [outputs.http.headers]
    Content-Type = "application/json"
    X-Telegraf-Token = "${TELEGRAF_TOKEN}"
    X-Agent-ID = "${TELEGRAF_AGENT_ID}"

# -----------------------------------------------------------------------------
# INFLUXDB OUTPUT (Optional - Uncomment when ready)
# -----------------------------------------------------------------------------
# Secondary output: sends numeric metrics directly to InfluxDB for time-series
# analysis, anomaly detection baselines, and predictive analytics.
#
# [[outputs.influxdb_v2]]
#   # InfluxDB server URL
#   urls = ["${INFLUXDB_URL:=http://localhost:8086}"]
#
#   # Authentication token
#   token = "${INFLUXDB_TOKEN}"
#
#   # Organization name
#   organization = "${INFLUXDB_ORG:=aiops}"
#
#   # Destination bucket
#   bucket = "${INFLUXDB_BUCKET:=telemetry}"
#
#   # Only send numeric metrics (exclude logs)
#   [outputs.influxdb_v2.tagpass]
#     input = ["cpu", "mem", "disk", "docker", "system", "net"]

# =============================================================================
# INPUT PLUGINS - LOGS
# =============================================================================

# -----------------------------------------------------------------------------
# DOCKER CONTAINER LOGS
# -----------------------------------------------------------------------------
# Collects logs from all running Docker containers via Docker API
[[inputs.docker_log]]
  # Docker daemon socket
  # Docker Desktop for Mac: use the default or ~/.docker/run/docker.sock
  endpoint = "unix:///var/run/docker.sock"

  # Read logs from the beginning (false = only new logs)
  from_beginning = false

  # Timeout for Docker API calls
  timeout = "5s"

  # Container name filters (empty = all containers)
  # Uncomment and modify to filter specific containers:
  # container_name_include = ["api", "worker", "frontend"]
  # container_name_exclude = ["telegraf"]

  # Container state filters
  container_state_include = ["running"]

  # Source tag for routing in backend
  [inputs.docker_log.tags]
    log_type = "docker"
    input = "docker_log"

# -----------------------------------------------------------------------------
# MACOS SYSTEM LOGS
# -----------------------------------------------------------------------------
# Executes a script to stream macOS unified logs
[[inputs.exec]]
  # Path to the macOS log streaming script
  # Adjust path based on your installation location
  commands = ["${TELEGRAF_SCRIPTS_PATH}/macos_log.sh"]

  # Execution interval (how often to collect new logs)
  interval = "10s"

  # Timeout for script execution
  timeout = "9s"

  # Output data format
  data_format = "influx"

  # Override metric name for backend routing
  name_override = "macos_log"

  [inputs.exec.tags]
    log_type = "system"
    os = "macos"
    input = "exec"

# -----------------------------------------------------------------------------
# APPLICATION LOG FILES
# -----------------------------------------------------------------------------
# Tails application log files from common macOS locations
[[inputs.tail]]
  # Log file patterns to monitor
  # Add or modify paths based on your applications
  files = [
    "/var/log/*.log",
    "/usr/local/var/log/*.log",
    "/opt/homebrew/var/log/*.log",
    "${HOME}/Library/Logs/*.log"
  ]

  # Start reading from end of file (false = only new lines)
  from_beginning = false

  # File watching method
  # "inotify" is more efficient but may not work on all systems
  watch_method = "poll"

  # Data format for the log lines
  data_format = "value"
  data_type = "string"

  # Override metric name for backend routing
  name_override = "macos_log"

  # Add file path as a tag
  path_tag = "log_file"

  [inputs.tail.tags]
    log_type = "application"
    input = "tail"

# =============================================================================
# INPUT PLUGINS - METRICS
# =============================================================================

# -----------------------------------------------------------------------------
# DOCKER CONTAINER METRICS
# -----------------------------------------------------------------------------
# Collects resource usage metrics from Docker containers
[[inputs.docker]]
  # Docker daemon socket
  endpoint = "unix:///var/run/docker.sock"

  # Gather metrics from running containers only
  gather_services = false

  # Container filters (empty = all containers)
  container_names = []

  # Collect per-device stats (network, blkio)
  perdevice = true

  # Collect total stats across all devices
  total = false

  # Include container name as short ID
  container_name_include = []
  container_name_exclude = []

  # Timeout for Docker API calls
  timeout = "5s"

  # Metrics to exclude (reduce noise)
  # Uncomment to exclude specific metrics:
  # fielddrop = ["container_id"]

  [inputs.docker.tags]
    input = "docker"

# -----------------------------------------------------------------------------
# CPU METRICS
# -----------------------------------------------------------------------------
[[inputs.cpu]]
  # Collect per-CPU metrics
  percpu = true

  # Collect total CPU metrics
  totalcpu = true

  # Collect CPU time metrics
  collect_cpu_time = false

  # Report active CPU state
  report_active = false

  [inputs.cpu.tags]
    input = "cpu"

# -----------------------------------------------------------------------------
# MEMORY METRICS
# -----------------------------------------------------------------------------
[[inputs.mem]]
  [inputs.mem.tags]
    input = "mem"

# -----------------------------------------------------------------------------
# DISK METRICS
# -----------------------------------------------------------------------------
[[inputs.disk]]
  # Mount points to collect
  # Empty = all mount points
  mount_points = []

  # Filesystems to ignore
  ignore_fs = [
    "tmpfs",
    "devtmpfs",
    "devfs",
    "iso9660",
    "overlay",
    "aufs",
    "squashfs",
    "autofs",
    "nfs",
    "nfs4"
  ]

  [inputs.disk.tags]
    input = "disk"

# -----------------------------------------------------------------------------
# DISK I/O METRICS
# -----------------------------------------------------------------------------
[[inputs.diskio]]
  # Devices to collect (empty = all)
  devices = []

  # Skip serial number collection (can be slow)
  skip_serial_number = true

  [inputs.diskio.tags]
    input = "diskio"

# -----------------------------------------------------------------------------
# SYSTEM METRICS
# -----------------------------------------------------------------------------
[[inputs.system]]
  [inputs.system.tags]
    input = "system"

# -----------------------------------------------------------------------------
# NETWORK METRICS
# -----------------------------------------------------------------------------
[[inputs.net]]
  # Interfaces to collect (empty = all non-loopback)
  interfaces = []

  # Ignore specific interfaces
  ignore_protocol_stats = false

  [inputs.net.tags]
    input = "net"

# -----------------------------------------------------------------------------
# PROCESS METRICS (Optional - can be noisy)
# -----------------------------------------------------------------------------
# Uncomment to collect process-level metrics
# [[inputs.processes]]
#   [inputs.processes.tags]
#     input = "processes"

# =============================================================================
# PROCESSORS (Optional)
# =============================================================================

# -----------------------------------------------------------------------------
# RENAME PROCESSOR
# -----------------------------------------------------------------------------
# Renames metrics or tags for consistency
# [[processors.rename]]
#   [[processors.rename.replace]]
#     measurement = "docker_container_cpu"
#     dest = "container.cpu"

# -----------------------------------------------------------------------------
# FILTER PROCESSOR
# -----------------------------------------------------------------------------
# Filters out unwanted metrics
# [[processors.filter]]
#   namepass = ["cpu", "mem", "disk", "docker*", "macos_log"]
